import os
from typing import TypedDict, List, Optional
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import AnyMessage

# Load environment variables
load_dotenv()

# --- State Definition ---
Messages = List[BaseMessage] # Use BaseMessage for broader compatibility

class SimpleTutorState(TypedDict):
    """
    Represents the state of our simple tutor graph.
    """
    problem: str
    user_answer: Optional[str]
    # Use MessagesPlaceholder for history management in LangGraph
    chat_history: Messages

    # Fields generated by the graph
    correct_answer: Optional[str]
    is_correct: Optional[bool]
    feedback: Optional[str]
    hint: Optional[str]
    error: Optional[str]

# --- Initialize LLM ---
# Use a cost-effective model for now
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# --- Tool Definition (Placeholder for now) ---
# We'll define a tool for solving/checking math problems later

# --- Node Functions (Placeholders) ---
def generate_correct_answer(state: SimpleTutorState) -> dict:
    print("--- Node: generate_correct_answer ---")
    # TODO: Implement tool/LLM call to find the correct answer
    problem = state["problem"]
    # Placeholder logic
    try:
        # Simulate solving simple equations like 'x+1=5' -> 'x=4'
        if '=' in problem:
             # Basic symbolic solve (very limited)
             import sympy
             x = sympy.symbols('x')
             try:
                 lhs, rhs = map(sympy.sympify, problem.split('='))
                 solution = sympy.solve(sympy.Eq(lhs, rhs), x)
                 if solution:
                     correct_ans_str = f"x = {solution[0]}"
                 else:
                     correct_ans_str = "Couldn't solve symbolically (basic)."
             except Exception as e:
                 correct_ans_str = f"Symbolic solve error: {e}"
        else:
             correct_ans_str = "Problem format not recognized for solving."

        return {"correct_answer": correct_ans_str}
    except Exception as e:
        return {"error": f"Error in generate_correct_answer: {str(e)}"}


def assess_answer(state: SimpleTutorState) -> dict:
    print("--- Node: assess_answer ---")
    # TODO: Compare user_answer with correct_answer
    user_ans = state.get("user_answer")
    correct_ans = state.get("correct_answer")
    if not user_ans or not correct_ans:
        # If no user answer provided yet, skip assessment
        if not user_ans:
            return {"feedback": "No user answer provided to assess."}
        return {"feedback": "Cannot assess answer without both user answer and correct answer."}

    # Simple comparison for now (needs improvement for math equivalence)
    is_correct = (user_ans.strip().lower() == correct_ans.strip().lower())
    feedback = "Correct!" if is_correct else "That doesn't seem right. Try again or ask for a hint."

    return {"is_correct": is_correct, "feedback": feedback}

def generate_hint(state: SimpleTutorState) -> dict:
    print("--- Node: generate_hint ---")
    # TODO: Implement LLM call for hint generation
    problem = state["problem"]
    history = state["chat_history"]
    # Placeholder
    hint_text = f"Hint for '{problem}': Maybe check your calculation steps?"
    # Add hint as AI message to history
    # Ensure history is a list before appending
    current_history = list(history) if history else []
    new_history = current_history + [AIMessage(content=hint_text)]
    return {"hint": hint_text, "chat_history": new_history}

def generate_solution_response(state: SimpleTutorState) -> dict:
    print("--- Node: generate_solution_response ---")
    correct_ans = state.get("correct_answer", "No solution generated.")
    response = f"The correct answer is: {correct_ans}"
     # Add response as AI message to history
    current_history = list(state["chat_history"]) if state.get("chat_history") else []
    new_history = current_history + [AIMessage(content=response)]
    return {"feedback": response, "chat_history": new_history} # Use feedback field for final output

# --- Graph Definition ---
builder = StateGraph(SimpleTutorState)

# Add nodes
builder.add_node("generate_correct_answer", generate_correct_answer)
builder.add_node("assess_answer", assess_answer)
builder.add_node("generate_hint", generate_hint)
builder.add_node("generate_solution_response", generate_solution_response)

# Define edges (Simplified flow for now, will add interrupts later)
builder.set_entry_point("generate_correct_answer")

# Conditional logic after assessment
def decide_after_assessment(state: SimpleTutorState):
    # Ensure assessment happened and feedback exists
    if state.get("feedback") == "No user answer provided to assess.":
        print("--- Branch: No user answer yet, ending (will be interrupt later) ---")
        return "__end__" # Use __end__ directly for clarity
    if state.get("is_correct") is True:
        print("--- Branch: Correct answer, ending ---")
        return "__end__"
    else:
        # For now, just end. Later, wait for user input (hint/retry)
        print("--- Branch: Incorrect answer, ending (will be interrupt later) ---")
        return "__end__"

builder.add_edge("generate_correct_answer", "assess_answer")
builder.add_conditional_edges(
    "assess_answer",
    decide_after_assessment,
    {"__end__": END} # Map return values to node names or END
)

# For hint/solution paths (will be triggered differently later with interrupts)
# builder.add_edge("generate_hint", END)
# builder.add_edge("generate_solution_response", END)


# Compile the graph
# Add checkpointer for memory persistence if needed later
app = builder.compile()

print("Simple Tutor Graph Compiled.")

# Example of how to run (will be interactive later)
if __name__ == "__main__":
    import asyncio
    import pprint

    async def run_example():
        # Example 1: Correct Answer
        initial_state_correct = {
            "problem": "x + 5 = 10",
            "user_answer": "x = 5", # Correct answer
            "chat_history": [HumanMessage(content="Solve x + 5 = 10. My answer is x = 5.")]
        }
        print("\n--- Running Example 1 (Correct Answer) ---")
        final_state_correct = None
        async for event in app.astream(initial_state_correct):
            print(pprint.pformat(event))
            # Capture the final state from the last event
            if event.get("event") == "end":
                final_state_correct = event.get("data", {}).get("output")
        print("--- Final State (Correct) ---")
        print(pprint.pformat(final_state_correct))


        # Example 2: Incorrect Answer
        initial_state_wrong = {
            "problem": "x + 5 = 10",
            "user_answer": "x = 6", # Incorrect answer
             "chat_history": [HumanMessage(content="Solve x + 5 = 10. My answer is x = 6.")]
        }
        print("\n--- Running Example 2 (Incorrect Answer) ---")
        final_state_wrong = None
        async for event in app.astream(initial_state_wrong):
            print(pprint.pformat(event))
            if event.get("event") == "end":
                 final_state_wrong = event.get("data", {}).get("output")
        print("--- Final State (Incorrect) ---")
        print(pprint.pformat(final_state_wrong))

        # Example 3: No user answer yet (relevant for interactive flow)
        initial_state_no_answer = {
            "problem": "x + 5 = 10",
            "user_answer": None, # No answer
             "chat_history": [HumanMessage(content="Solve x + 5 = 10.")]
        }
        print("\n--- Running Example 3 (No Answer) ---")
        final_state_no_answer = None
        async for event in app.astream(initial_state_no_answer):
            print(pprint.pformat(event))
            if event.get("event") == "end":
                 final_state_no_answer = event.get("data", {}).get("output")
        print("--- Final State (No Answer) ---")
        print(pprint.pformat(final_state_no_answer))


    asyncio.run(run_example())
